---
title: |
  <center> DATA 698 </center>
  <center> Capstone Project </center>
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document:
    toc: yes
    toc_depth: 4
---

\begin{center}
\bigskip
\bigskip
\bigskip
Prepared for:\\
\medskip
Prof. Dr. Nasrin Khansari\\
\smallskip
City University of New York, School of Professional Studies\\
\bigskip
Prepared by:\\
\medskip
Sie Siong Wong\\ 
\smallskip
Mario Pena\\
\smallskip
Joseph Shi\\  
\end{center}

\pagebreak

```{R message=FALSE, warning=FALSE, echo=FALSE}
options(warn=-1)
if(!require('png')) (install.packages('png'))
if(!require('grid')) (install.packages('grid'))
if(!require('tidyverse')) (install.packages('tidyverse'))
if(!require('academictwitteR')) (install.packages('academictwitteR'))
if(!require('tidyr')) (install.packages('tidyr'))
if(!require('readr')) (install.packages('readr'))
if(!require('dplyr')) (install.packages('dplyr'))
if(!require('zipcodeR')) (install.packages('zipcodeR'))
if(!require('RSocrata')) (install.packages('RSocrata'))
if(!require('future.apply')) (install.packages('future.apply'))
if(!require('syuzhet')) (install.packages('syuzhet'))
if(!require('tm')) (install.packages('tm'))
if(!require('lda')) (install.packages('lda'))
if(!require('anytime')) (install.packages('anytime'))
if(!require('SnowballC')) (install.packages('SnowballC'))
if(!require('topicmodels')) (install.packages('topicmodels'))
if(!require('wordcloud')) (install.packages('wordcloud'))
if(!require('tidytext')) (install.packages('tidytext'))
```

\newpage

# Abstract

\newpage

# Introduction

\newpage

# Literature Review

\newpage

# Theory and Hypotheses

```{r warning=FALSE, message=FALSE, echo=FALSE, fig.cap="Architecture of the Proposed Methodology", fig.width=10, fig.height=10}
img <- readPNG("C:\\Users\\wongs34\\Documents\\School\\DATA 698\\Proposal\\Architecture of the Proposed Methodology.png")
grid.raster(img)
```

\newpage

# Data and Methods

\newpage

# Results

\newpage

# Discussion

\newpage

# Conclusion

\newpage

# References

1. Kshirsagar V. (2019, December 24). *Detecting Hate tweets — Twitter Sentiment Analysis*. Towards Data Science. Retrieved from https://towardsdatascience.com/detecting-hate-tweets-twitter-sentiment-analysis-780d8a82d4f6

2. Crabb et al. (2019, May 28). *Classifying Hate Speech: an overview*. Towards Data Science. Retrieved from https://towardsdatascience.com/classifying-hate-speech-an-overview-d307356b9eba

3. Pang, G. (2022, March 4). *Deep Learning for Hate Speech Detection: A Large-scale Empirical Evaluation*. Towards Data Science. Retrieved from https://towardsdatascience.com/deep-learning-for-hate-speech-detection-a-large-scale-empirical-evaluation-92831ded6bb6

3. Kim, H. (2022). *Sentiment Analysis: Limits and Progress of the Syuzhet Package and Its Lexicons*. Texas A&M University. Retrieved from http://www.digitalhumanities.org/dhq/vol/16/2/000612/000612.html

4. Williams et al. (2019, July 23). *Hate in the Machine: Anti-Black and Anti-Muslim Social Media Posts as Predictors of Offline Racially and Religiously Aggravated Crime*. The British Journal of Criminology. Retrieved from https://academic.oup.com/bjc/article/60/1/93/5537169

5. Lee et al. (2022, January). *Racism Detection by Analyzing Differential Opinions Through Sentiment Analysis of Tweets Using Stacked Ensemble GCR-NN Model*. ResearchGate. Retrieved from https://www.researchgate.net/publication/357916429_Racism_Detection_by_Analyzing_Differential_Opinions_Through_Sentiment_Analysis_of_Tweets_Using_Stacked_Ensemble_GCR-NN_Model

6. Matsaki L. (2018, September 26). *To Break a Hate-Speech Detection Algorithm, Try 'Love'*. Wired.  Retrieved from https://www.wired.com/story/break-hate-speech-algorithm-try-love/

7. Rizwan et al. (2020, January). *Hate-Speech and Offensive Language Detection in Roman Urdu*. ACL Anthology. Retrieved from https://aclanthology.org/2020.emnlp-main.197.pdf

8. Miró-Llinares et al. (2018, November 15). *Hate is in the air! But where? Introducing an algorithm to detect hate speech in digital microenvironments*. Springer Link. Retrieved from https://link.springer.com/article/10.1186/s40163-018-0089-1

9. Curiel et al. (2020, April 02). *Crime and its fear in social media*. Nature. Retrieved from https://www.nature.com/articles/s41599-020-0430-7#Sec8

10. Sandagiri et al. (2021, January 19). *Detecting Crime Related Twitter Posts using Artificial Neural Networks based Approach*. IEEE Xplore. Retrieved from https://ieeexplore.ieee.org/document/9325485

11. Kumar, A. (2022, March 20). *Hate Speech Detection Using Machine Learning*. Vitalflux. Retrieved from https://vitalflux.com/hate-speech-detection-using-machine-learning/#:~:text=The%20techniques%20for%20detecting%20hate,to%20find%20patterns%20in%20data.

12. Alnazzawi, N. (2022, May 24). *Using Twitter to Detect Hate Crimes and Their Motivations: The HateMotiv Corpus*. MDPI. Retrieved from https://www.mdpi.com/2306-5729/7/6/69

13. Kocon et al. (2021, June 03). *Offensive, aggressive, and hate speech analysis: From data-centric to human-centered approach*. ScienceDirect. Retrieved from https://www.sciencedirect.com/science/article/pii/S0306457321001333

14. Shead, S. (2020, November 19). *Facebook claims A.I. now detects 94.7% of the hate speech that gets removed from its platform*. CNBC. Retrieved from https://www.cnbc.com/2020/11/19/facebook-says-ai-detects-94point7percent-of-hate-speech-removed-from-platform.html#:~:text=Facebook%20announced%20Thursday%20that%20artificial,and%20just%2024%25%20in%202017.

15. Williams, M. & Burnap, P. (2015, June 25 ). *Cyberhate on Social Media in the Aftermath of Woolwich: A Case Study in Computational Criminology and Big Data*. British Journal of Criminology. Retrieved from https://academic.oup.com/bjc/article/56/2/211/2462519

16. Hanes, E. & Machin, S. (2013, September). *Hate Crime in the Wake of Terror Attacks: Evidence from 7/7 and 9/11*. Journal of Contemporary Criminal Justice. Retrieved from https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1038.9462&rep=rep1&type=pdf

17. Hawdon et al. (2016, July 13). *Exposure to Online Hate in Four Nations: A Cross-National Consideration*. ResearchGate. Retrieved from https://www.researchgate.net/publication/303480309_Exposure_to_Online_Hate_in_Four_Nations_A_Cross-National_Consideration

18. Starosta A. (2019, February 15). *CBuilding NLP Classifiers Cheaply With Transfer Learning and Weak Supervision. Medium*. Retrieved from https://medium.com/sculpt/a-technique-for-building-nlp-classifiers-efficiently-with-transfer-learning-and-weak-supervision-a8e2f21ca9c8

19. Chetty N. & Alathur S. (2018, May 8). *Hate speech review in the context of online social networks*. Retrieved from https://www.sciencedirect.com/science/article/pii/S1359178917301064

20. Miró-Llinares F. & Rodriguez-Sala J.J. (2016, July). *Cyber Hate Speech on Twitter: Analyzing Disruptive Events from Social Media to Build a Violent Communication and Hate Speech taxonomy*. ResearchGate. Retrieved from https://www.researchgate.net/publication/308487177_Cyber_hate_speech_on_twitter_Analyzing_disruptive_events_from_social_media_to_build_a_violent_communication_and_hate_speech_taxonomy

21. Chen et al. (2015, June 8). *Crime prediction using Twitter sentiment and weather*. IEEE Xplore.  Retrieved from https://ieeexplore.ieee.org/abstract/document/7117012/authors#authors

\newpage

# Appendix A1. Twitter Data Collection

```{r}

# To run below codes for collecting tweets, first you'll need to create a .Renviron file by running
# set_bearer(), then store their bearer token in one line in the file like this
# TWITTER_BEARER = 'bearer token', close the file, and restart R session. 

# Run this code to use API credentials saved in the .Renviron file connects to the Twitter to get tweets data.
get_bearer()

# Function to get all tweets from year 2020 to 2022 by borough
get_tweets <- function(borough) {
  get_tweets <- get_all_tweets(
    query = "",
    n = 1000000,
    start_tweets = "2019-01-01T10:00:00Z",
    end_tweets = "2021-12-31T10:00:00Z",
    country = "US", 
    place = borough,
    lang = "en",
    page_n = 500
    
  )
  
  return(get_tweets)
}

# Function to select variables, transform date and coordinates,
# extract list-like columns, longitude, and latitude into new columns
extract_cols <- function(tweets, borough) {
  all_tweets <- cbind(tweets %>%
                        select(id,
                               created_at,
                               text,
                               source,
                               possibly_sensitive),
                      as.data.frame(tweets$geo$coordinates), 
                      as.data.frame(tweets$public_metrics))
  
  
  all_tweets <- all_tweets %>% 
    mutate(coordinates = as.character(coordinates))
  
  all_tweets <- all_tweets %>% 
    mutate(created_at = as.Date(created_at))
  
  all_tweets <- all_tweets %>% 
    mutate(longitude = parse_number(sub("[^-\\d]+", "", coordinates)))
  
  all_tweets <- all_tweets %>% 
    mutate(latitude = parse_number(sub("[^, \\d]+", "", coordinates)))
  
  all_tweets$borough <- borough
  
  return(all_tweets)
}

# Get historical tweets for each borough
m_twt_raw <- get_tweets("Manhattan, NY")
q_twt_raw <- get_tweets("Queens, NY")
k_twt_raw <- get_tweets("Brooklyn, NY")
x_twt_raw <- get_tweets("Bronx, NY")
s_twt_raw <- get_tweets("Staten Island, NY")

# Clean tweets data for each borough
m_twt_clean <- extract_cols(m_twt_raw, "MANHATTAN")
q_twt_clean <- extract_cols(q_twt_raw, "QUEENS")
k_twt_clean <- extract_cols(k_twt_raw, "BROOKLYN")
x_twt_clean <- extract_cols(x_twt_raw, "BRONX")
s_twt_clean <- extract_cols(s_twt_raw, "STATEN ISLAND")

# Combine borough data sets
tweets_data <- rbind(m_twt_clean,q_twt_clean,k_twt_clean,x_twt_clean,s_twt_clean)

# Read data from Github.
# As it takes time run above codes to collect NYC tweets data and also quota limitation per month to get # tweets data from Twitter, we uploaded the combined data we collect from running above codes to a Github # repo for convenient to read into R at any time.
tweets_data <- read.csv("https://raw.githubusercontent.com/SieSiongWong/DATA-698/master/Data/all_boroughs_2019_2021.csv", header=TRUE, sep=",")

```

\newpage

# Appendix A2. Crime Data Collection

```{r}

# Collect crime data from year 2019 to 2021.
crime_data <- read.socrata("https://data.cityofnewyork.us/resource/qgea-i56i.json?$select=cmplnt_fr_dt,addr_pct_cd,ofns_desc,pd_desc,susp_age_group,boro_nm,susp_race,susp_sex,latitude,longitude,vic_age_group,vic_race,vic_sex&$where=cmplnt_fr_dt between \'2019\' and \'2022\'")

```

\newpage

# Appendix B. Data Preprocessing

```{r}

# Filter hate related offensive corresponding description
hate_crime <- filter(crime_data, grepl('ASSAULT|HARRASSMENT', ofns_desc))

# Convert to date type
hate_crime$cmplnt_fr_dt <- as.Date(hate_crime$cmplnt_fr_dt)

# Remove meaningless characters and symbols in tweets text
tweets_data$text <- gsub("&amp","", tweets_data$text)
tweets_data$text <- gsub("(RT)((?:\\b\\w*@\\w+)+)","", tweets_data$text)
tweets_data$text <- gsub("^RT","", tweets_data$text)
tweets_data$text <- gsub("@\\w+","", tweets_data$text)
tweets_data$text <- gsub("[[:punct:]]","", tweets_data$text)
tweets_data$text <- gsub("[[:digit:]]+\\s","", tweets_data$text)
tweets_data$text <- gsub("http\\w+","", tweets_data$text)
tweets_data$text <- gsub("[ \t]{2,}"," ", tweets_data$text)

# Remove all non-ASCII characters 
tweets_data$text <- iconv(tweets_data$text, "UTF-8", "ASCII", sub="")

# Tokenize the text and see frequency of words.
tweets_data %>% 
  unnest_tokens(word, text)%>%
  anti_join(stop_words) %>%
  count(word, sort=TRUE) 

# We can see that words such as "ny, york, im, brooklyn, manhattan, bronx, nyc, queens, ave, 
# newyork" not pertaining to hatism topic, so we remove them.
tweets_data <- tweets_data %>% mutate(text=tolower(text))
tweets_data$text <- gsub("\\bny\\b|\\byork\\b|\\bim\\b|\\bbrooklyn\\b
                        |\\bmanhattan\\b|\\bbronx\\b|\\bqueens\\b|\\bnyc\\b
                        |\\bave\\b|\\bnewyork\\b|\\bstaten\\b|\\bisland\\b
                        |\\bnewyorkcity\\b|\\bjamaica\\b|\\bflushing\\b",
                        "", tweets_data$text)

```

\newpage

# Appendix C. Data Exploration

```{r}




```

\newpage

# Appendix D. Data Preparation

```{r}

# Get negative emotion score tweets
tweets_vector <- as.vector(tweets_data$text)
emotion_score <- get_sentiment(tweets_vector)
tweets_negative <- cbind(tweets_data, emotion_score) %>% filter(emotion_score < 0)


# Function to get zip code from latitude and longitude
search_zip2 <- function(latitude, longitude) {
    
  return(if(is.na(latitude) | is.na(longitude)){''} 
         else {search_radius(latitude, longitude, radius = 0.7)$zipcode[1]})
  
}

# Use parallel processing method to run the function to get zip code
data$zipcode <- future_mapply(search_zip2,test2$latitude, test2$longitude)

```

\newpage

# Appendix E. Build Models

```{r}
```

\newpage

# Appendix F. Model Evaluation

```{r}
```


